{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simple-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print(\"Device:\", tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "married-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "GCS_PATH = \"/data/datasets/saket/SeeingThroughFogData/train_clear_day/*.swedentfrecord\"\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = [1024, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wireless-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets import dataset_utils\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern, reader,\n",
    "              split_to_sizes, items_to_descriptions, num_classes):\n",
    "    \"\"\"Gets a dataset tuple with instructions for reading Pascal VOC dataset.\n",
    "\n",
    "    Args:\n",
    "      split_name: A train/test split name.\n",
    "      dataset_dir: The base directory of the dataset sources.\n",
    "      file_pattern: The file pattern to use when matching the dataset sources.\n",
    "        It is assumed that the pattern contains a '%s' string so that the split\n",
    "        name can be inserted.\n",
    "      reader: The TensorFlow reader type.\n",
    "\n",
    "    Returns:\n",
    "      A `Dataset` namedtuple.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if `split_name` is not a valid train/test split.\n",
    "    \"\"\"\n",
    "    if split_name not in split_to_sizes:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "    file_pattern = os.path.join(dataset_dir, file_pattern)\n",
    "\n",
    "    # Allowing None in the signature so that dataset_factory can use the default.\n",
    "    if reader is None:\n",
    "        reader = tf.TFRecordReader\n",
    "\n",
    "    # change\n",
    "    keys_to_features= {\n",
    "        #'key': tf.VarLenFeature(dtype=tf.int64),\n",
    "        #'name': tf.FixedLenFeature((), tf.string),\n",
    "        'image/cam_stereo_left_lut': tf.FixedLenFeature((), tf.string),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n",
    "        'image/object/class/text': tf.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/angle': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/truncation': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/occlusion': tf.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/object/bbox3d/height': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/width': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/length': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/x': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/y': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/z': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/alpha3d': tf.VarLenFeature(dtype=tf.float32),\n",
    "\n",
    "        #'image/key': tf.FixedLenFeature((), tf.string),\n",
    "        'image/shape/cam_stereo_left_lut': tf.FixedLenFeature([3], tf.int64),\n",
    "        #'lidar/point_key': tf.VarLenFeature(dtype=tf.float32),\n",
    "        #'lidar/shape': tf.VarLenFeature(dtype=tf.int64),\n",
    "        #'gated/key': tf.FixedLenFeature((), tf.string),\n",
    "        #'gated/shape': tf.VarLenFeature(dtype=tf.int64),\n",
    "    }\n",
    "    # change\n",
    "    items_to_handlers = {\n",
    "        #'key': slim.tfexample_decoder.Tensor('key'),\n",
    "        #'name': slim.tfexample_decoder.Tensor('name'),\n",
    "        'image': slim.tfexample_decoder.Image('image/cam_stereo_left_lut'),\n",
    "        'shape': slim.tfexample_decoder.Tensor('image/shape/cam_stereo_left_lut'),\n",
    "        'class/text': slim.tfexample_decoder.Tensor('image/object/class/text'),\n",
    "        'object/bbox': slim.tfexample_decoder.BoundingBox(\n",
    "                ['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/'),\n",
    "        #'bbox/angle': slim.tfexample_decoder.Tensor('image/object/bbox/angle'),\n",
    "        #'object/label': slim.tfexample_decoder.Tensor('image/object/bbox/label'),\n",
    "        #'object/truncation': slim.tfexample_decoder.Tensor('image/object/truncation'),\n",
    "        #'object/occlusion': slim.tfexample_decoder.Tensor('image/object/occlusion'),\n",
    "        #'object/bbox3d': slim.tfexample_decoder.BoundingBox(\n",
    "        #    ['height', 'width', 'length', 'x','y','z'],'image/object/object/bbox3d/'),\n",
    "        #'bbox3d/alpha3d': slim.tfexample_decoder.Image('image/object/object/bbox3d/alpha3d'),\n",
    "        \n",
    "    }\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "    print(decoder)\n",
    "\n",
    "    labels_to_names = None\n",
    "    if dataset_utils.has_labels(dataset_dir):\n",
    "        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n",
    "    # else:\n",
    "    #     labels_to_names = create_readable_names_for_imagenet_labels()\n",
    "    #     dataset_utils.write_label_file(labels_to_names, dataset_dir)\n",
    "\n",
    "    return slim.dataset.Dataset(\n",
    "            data_sources=file_pattern,\n",
    "            reader=reader,\n",
    "            decoder=decoder,\n",
    "            num_samples=split_to_sizes[split_name],\n",
    "            items_to_descriptions=items_to_descriptions,\n",
    "            num_classes=num_classes,\n",
    "            labels_to_names=labels_to_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scenic-tourism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.slim.python.slim.data.tfexample_decoder.TFExampleDecoder object at 0x7f7386be8510>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.slim.python.slim.data.dataset.Dataset at 0x7f7386be8e90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 8\n",
    "file_pattern = '*.swedentfrecord'\n",
    "split_name = \"train_clear_day\"\n",
    "dataset_dir = \"/data/datasets/saket/SeeingThroughFogData/train_clear_day\"\n",
    "reader = tf.TFRecordReader\n",
    "split_to_sizes = {\n",
    "    'train_clear_day': 2183,\n",
    "    #'validation': 50000,\n",
    "}\n",
    "items_to_descriptions = {\n",
    "    'image_data': 'A color image of varying height and width.',\n",
    "    'gated_data': 'Gated camera images of varying height and width.',\n",
    "    'lidar_data': 'Lidar Data .bin files.',\n",
    "    'image_shape': 'Shape of the image',\n",
    "    'lidar_shape': 'Shape of the Lidar data',\n",
    "    'gated_shape': 'Shape of the Gated camera image',\n",
    "    'label': 'Common labels for image, gated, and lidar data',\n",
    "    'name': 'Entry ID (Files name used for training)',\n",
    "    'total_id': 'Total ID',\n",
    "}\n",
    "\n",
    "\n",
    "get_split(split_name, dataset_dir, file_pattern, reader,\n",
    "              split_to_sizes, items_to_descriptions, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "important-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datasets import cifar10\n",
    "from datasets import imagenet\n",
    "#change\n",
    "from datasets import fullseeingthroughfogdataset\n",
    "\n",
    "from datasets import pascalvoc_2007\n",
    "from datasets import pascalvoc_2012\n",
    "\n",
    "# change\n",
    "datasets_map = {\n",
    "    'cifar10': cifar10,\n",
    "    'imagenet': imagenet,\n",
    "    'pascalvoc_2007': pascalvoc_2007,\n",
    "    'pascalvoc_2012': pascalvoc_2012,\n",
    "    'fullseeingthroughfogdataset': fullseeingthroughfogdataset,\n",
    "}\n",
    "\n",
    "\n",
    "def get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n",
    "    \"\"\"Given a dataset name and a split_name returns a Dataset.\n",
    "\n",
    "    Args:\n",
    "        name: String, the name of the dataset.\n",
    "        split_name: A train/test split name.\n",
    "        dataset_dir: The directory where the dataset files are stored.\n",
    "        file_pattern: The file pattern to use for matching the dataset source files.\n",
    "        reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n",
    "            reader defined by each dataset is used.\n",
    "    Returns:\n",
    "        A `Dataset` class.\n",
    "    Raises:\n",
    "        ValueError: If the dataset `name` is unknown.\n",
    "    \"\"\"\n",
    "    #split_name = \"train_clear_day\"\n",
    "    #dataset_dir = \"/data/datasets/saket/SeeingThroughFogData\"\n",
    "\n",
    "    #print(datasets_map[name].get_split(file_pattern))\n",
    "\n",
    "    if name not in datasets_map:\n",
    "        raise ValueError('Name of dataset unknown %s' % name)\n",
    "    return datasets_map[name].get_split(split_name,\n",
    "                                        dataset_dir,\n",
    "                                        file_pattern,\n",
    "                                        reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "usual-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Num GPUs Available:  0\n",
      "WARNING:tensorflow:From <ipython-input-1-a5c4cd3199c3>:205: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.create_global_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.464266 139898556168000 deprecation.py:323] From <ipython-input-1-a5c4cd3199c3>:205: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.create_global_step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/datasets/fullseeingthroughfogdataset_common.py:61: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.474351 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/datasets/fullseeingthroughfogdataset_common.py:61: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/datasets/fullseeingthroughfogdataset_common.py:63: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.476263 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/datasets/fullseeingthroughfogdataset_common.py:63: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/datasets/dataset_utils.py:111: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.478122 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/datasets/dataset_utils.py:111: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# =========================================================================== #\n",
      "# Training | Evaluation flags:\n",
      "# =========================================================================== #\n",
      "{'?': <absl.app.HelpFlag object at 0x7f3bf6541890>,\n",
      " 'adadelta_rho': <absl.flags._flag.Flag object at 0x7f3bf653d310>,\n",
      " 'adagrad_initial_accumulator_value': <absl.flags._flag.Flag object at 0x7f3bf653d3d0>,\n",
      " 'adam_beta1': <absl.flags._flag.Flag object at 0x7f3bf653d490>,\n",
      " 'adam_beta2': <absl.flags._flag.Flag object at 0x7f3bf653d550>,\n",
      " 'alsologtostderr': <absl.flags._flag.BooleanFlag object at 0x7f3c02fdf450>,\n",
      " 'batch_size': <absl.flags._flag.Flag object at 0x7f3bf6541590>,\n",
      " 'checkpoint_exclude_scopes': <absl.flags._flag.Flag object at 0x7f3bf6541850>,\n",
      " 'checkpoint_model_scope': <absl.flags._flag.Flag object at 0x7f3bf65417d0>,\n",
      " 'checkpoint_path': <absl.flags._flag.Flag object at 0x7f3bf6541750>,\n",
      " 'clone_on_cpu': <absl.flags._flag.BooleanFlag object at 0x7f3bf783c950>,\n",
      " 'dataset_dir': <absl.flags._flag.Flag object at 0x7f3bf6541350>,\n",
      " 'dataset_name': <absl.flags._flag.Flag object at 0x7f3bf6541150>,\n",
      " 'dataset_split_name': <absl.flags._flag.Flag object at 0x7f3bf6541290>,\n",
      " 'end_learning_rate': <absl.flags._flag.Flag object at 0x7f3bf653dd90>,\n",
      " 'ftrl_initial_accumulator_value': <absl.flags._flag.Flag object at 0x7f3bf653d790>,\n",
      " 'ftrl_l1': <absl.flags._flag.Flag object at 0x7f3bf653d850>,\n",
      " 'ftrl_l2': <absl.flags._flag.Flag object at 0x7f3bf653d910>,\n",
      " 'ftrl_learning_rate_power': <absl.flags._flag.Flag object at 0x7f3bf653d6d0>,\n",
      " 'gpu_memory_fraction': <absl.flags._flag.Flag object at 0x7f3bf653d0d0>,\n",
      " 'help': <absl.app.HelpFlag object at 0x7f3bf6541890>,\n",
      " 'helpfull': <absl.app.HelpfullFlag object at 0x7f3c02fd8510>,\n",
      " 'helpshort': <absl.app.HelpshortFlag object at 0x7f3bf65419d0>,\n",
      " 'helpxml': <absl.app.HelpXMLFlag object at 0x7f3c02fd8910>,\n",
      " 'ignore_missing_vars': <absl.flags._flag.BooleanFlag object at 0x7f3bf6541810>,\n",
      " 'label_smoothing': <absl.flags._flag.Flag object at 0x7f3bf653de50>,\n",
      " 'labels_offset': <absl.flags._flag.Flag object at 0x7f3bf6541390>,\n",
      " 'learning_rate': <absl.flags._flag.Flag object at 0x7f3bf653dcd0>,\n",
      " 'learning_rate_decay_factor': <absl.flags._flag.Flag object at 0x7f3bf653df10>,\n",
      " 'learning_rate_decay_type': <absl.flags._flag.Flag object at 0x7f3bf653dc10>,\n",
      " 'log_dir': <absl.flags._flag.Flag object at 0x7f3c02fdf510>,\n",
      " 'log_every_n_steps': <absl.flags._flag.Flag object at 0x7f3bf6538ed0>,\n",
      " 'logger_levels': <absl.logging._LoggerLevelsFlag object at 0x7f3c02fdf690>,\n",
      " 'logtostderr': <absl.flags._flag.BooleanFlag object at 0x7f3c02fd8d90>,\n",
      " 'loss_alpha': <absl.flags._flag.Flag object at 0x7f3bf6538890>,\n",
      " 'match_threshold': <absl.flags._flag.Flag object at 0x7f3bf6538a50>,\n",
      " 'max_number_of_steps': <absl.flags._flag.Flag object at 0x7f3bf65416d0>,\n",
      " 'model_name': <absl.flags._flag.Flag object at 0x7f3bf6541450>,\n",
      " 'momentum': <absl.flags._flag.Flag object at 0x7f3bf653d9d0>,\n",
      " 'moving_average_decay': <absl.flags._flag.Flag object at 0x7f3bf65410d0>,\n",
      " 'negative_ratio': <absl.flags._flag.Flag object at 0x7f3bf6538950>,\n",
      " 'num_classes': <absl.flags._flag.Flag object at 0x7f3bf6541210>,\n",
      " 'num_clones': <absl.flags._flag.Flag object at 0x7f3bf6538bd0>,\n",
      " 'num_epochs_per_decay': <absl.flags._flag.Flag object at 0x7f3bf653dfd0>,\n",
      " 'num_preprocessing_threads': <absl.flags._flag.Flag object at 0x7f3bf6538e10>,\n",
      " 'num_readers': <absl.flags._flag.Flag object at 0x7f3bf6538d90>,\n",
      " 'only_check_args': <absl.flags._flag.BooleanFlag object at 0x7f3c02fdff10>,\n",
      " 'op_conversion_fallback_to_while_loop': <absl.flags._flag.BooleanFlag object at 0x7f3c01805610>,\n",
      " 'opt_epsilon': <absl.flags._flag.Flag object at 0x7f3bf653d610>,\n",
      " 'optimizer': <absl.flags._flag.Flag object at 0x7f3bf653d250>,\n",
      " 'pdb': <absl.flags._defines.DEFINE_alias.<locals>._FlagAlias object at 0x7f3c0301b390>,\n",
      " 'pdb_post_mortem': <absl.flags._flag.BooleanFlag object at 0x7f3c03074250>,\n",
      " 'preprocessing_name': <absl.flags._flag.Flag object at 0x7f3bf6541510>,\n",
      " 'profile_file': <absl.flags._flag.Flag object at 0x7f3c03053fd0>,\n",
      " 'rmsprop_decay': <absl.flags._flag.Flag object at 0x7f3bf653db50>,\n",
      " 'rmsprop_momentum': <absl.flags._flag.Flag object at 0x7f3bf653da90>,\n",
      " 'run_with_pdb': <absl.flags._flag.BooleanFlag object at 0x7f3c03074150>,\n",
      " 'run_with_profiling': <absl.flags._flag.BooleanFlag object at 0x7f3c0304a1d0>,\n",
      " 'save_interval_secs': <absl.flags._flag.Flag object at 0x7f3bf6538fd0>,\n",
      " 'save_summaries_secs': <absl.flags._flag.Flag object at 0x7f3bf6538f90>,\n",
      " 'showprefixforinfo': <absl.flags._flag.BooleanFlag object at 0x7f3ca898c5d0>,\n",
      " 'stderrthreshold': <absl.logging._StderrthresholdFlag object at 0x7f3c02fdf8d0>,\n",
      " 'test_random_seed': <absl.flags._flag.Flag object at 0x7f3bfad84810>,\n",
      " 'test_randomize_ordering_seed': <absl.flags._flag.Flag object at 0x7f3bfad84b50>,\n",
      " 'test_srcdir': <absl.flags._flag.Flag object at 0x7f3bfadb2250>,\n",
      " 'test_tmpdir': <absl.flags._flag.Flag object at 0x7f3bfad6c690>,\n",
      " 'train_dir': <absl.flags._flag.Flag object at 0x7f3bf6538b10>,\n",
      " 'train_image_size': <absl.flags._flag.Flag object at 0x7f3bf6541650>,\n",
      " 'trainable_scopes': <absl.flags._flag.Flag object at 0x7f3bf65418d0>,\n",
      " 'use_cprofile_for_profiling': <absl.flags._flag.BooleanFlag object at 0x7f3c02fdfed0>,\n",
      " 'v': <absl.logging._VerbosityFlag object at 0x7f3c02fdf590>,\n",
      " 'verbosity': <absl.logging._VerbosityFlag object at 0x7f3c02fdf590>,\n",
      " 'weight_decay': <absl.flags._flag.Flag object at 0x7f3bf653d190>,\n",
      " 'xml_output_file': <absl.flags._flag.Flag object at 0x7f3bfad87450>}\n",
      "\n",
      "# =========================================================================== #\n",
      "# SSD net parameters:\n",
      "# =========================================================================== #\n",
      "{'anchor_offset': 0.5,\n",
      " 'anchor_ratios': [[2, 0.5],\n",
      "                   [2, 0.5, 3, 0.3333333333333333],\n",
      "                   [2, 0.5, 3, 0.3333333333333333],\n",
      "                   [2, 0.5, 3, 0.3333333333333333],\n",
      "                   [2, 0.5, 3, 0.3333333333333333],\n",
      "                   [2, 0.5],\n",
      "                   [2, 0.5]],\n",
      " 'anchor_size_bounds': [0.1, 0.9],\n",
      " 'anchor_sizes': [(20.48, 51.2),\n",
      "                  (51.2, 133.12),\n",
      "                  (133.12, 215.04),\n",
      "                  (215.04, 296.96),\n",
      "                  (296.96, 378.88),\n",
      "                  (378.88, 460.8),\n",
      "                  (460.8, 542.72)],\n",
      " 'anchor_steps': [8, 16, 32, 64, 128, 256, 512],\n",
      " 'feat_layers': ['block4',\n",
      "                 'block7',\n",
      "                 'block8',\n",
      "                 'block9',\n",
      "                 'block10',\n",
      "                 'block11',\n",
      "                 'block12'],\n",
      " 'feat_shapes': [(64, 64), (32, 32), (16, 16), (8, 8), (4, 4), (2, 2), (1, 1)],\n",
      " 'img_shape': (512, 512),\n",
      " 'no_annotation_label': 21,\n",
      " 'normalizations': [20, -1, -1, -1, -1, -1, -1],\n",
      " 'num_classes': 8,\n",
      " 'prior_scaling': [0.1, 0.1, 0.2, 0.2]}\n",
      "\n",
      "# =========================================================================== #\n",
      "# Training | Evaluation dataset files:\n",
      "# =========================================================================== #\n",
      "['/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000000.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000001.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000002.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000003.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000004.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000005.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000006.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000007.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000008.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000009.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000010.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000011.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000012.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000013.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000014.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000015.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000016.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000017.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000018.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000019.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000020.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000021.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000022.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000023.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000024.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000025.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000026.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000027.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000028.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000029.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000030.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000031.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000032.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000033.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000034.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000035.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000036.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000037.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000038.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000039.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000040.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000041.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000042.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000043.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000044.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000045.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000046.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000047.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000048.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000049.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000050.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000051.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000052.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000053.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000054.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000055.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000056.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000057.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000058.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000059.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000060.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000061.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000062.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000063.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000064.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000065.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000066.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000067.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000068.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000069.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000070.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000071.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000072.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000073.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000074.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000075.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000076.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000077.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000078.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000079.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000080.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000081.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000082.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000083.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000084.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000085.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000086.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000087.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000088.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000089.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000090.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000091.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000092.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000093.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000094.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000095.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000096.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000097.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000098.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000099.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000100.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000101.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000102.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000103.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000104.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000105.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000106.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000107.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000108.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000109.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000110.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000111.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000112.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000113.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000114.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000115.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000116.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000117.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000118.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000119.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000120.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000121.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000122.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000123.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000124.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000125.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000126.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000127.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000128.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000129.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000130.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000131.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000132.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000133.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000134.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000135.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000136.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000137.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000138.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000139.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000140.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000141.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000142.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000143.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000144.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000145.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000146.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000147.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000148.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000149.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000150.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000151.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000152.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000153.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000154.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000155.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000156.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000157.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000158.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000159.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000160.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000161.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000162.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000163.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000164.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000165.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000166.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000167.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000168.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000169.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000170.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000171.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000172.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000173.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000174.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000175.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000176.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000177.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000178.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000179.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000180.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000181.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000182.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000183.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000184.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000185.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000186.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000187.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000188.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000189.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000190.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000191.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000192.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000193.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000194.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000195.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000196.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000197.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000198.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000199.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000200.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000201.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000202.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000203.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000204.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000205.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000206.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000207.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000208.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000209.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000210.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000211.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000212.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000213.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000214.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000215.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000216.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000217.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000218.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000219.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000220.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000221.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000222.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000223.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000224.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000225.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000226.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000227.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000228.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000229.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000230.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000231.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000232.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000233.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000234.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000235.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000236.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000237.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000238.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000239.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000240.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000241.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000242.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000243.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000244.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000245.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000246.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000247.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000248.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000249.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000250.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000251.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000252.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000253.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000254.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000255.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000256.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000257.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000258.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000259.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000260.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000261.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000262.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000263.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000264.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000265.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000266.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000267.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000268.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000269.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000270.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000271.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000272.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000273.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000274.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000275.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000276.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000277.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000278.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000279.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000280.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000281.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000282.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000283.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000284.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000285.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000286.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000287.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000288.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000289.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000290.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000291.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000292.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000293.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000294.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000295.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000296.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000297.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000298.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000299.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000300.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000301.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000302.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000303.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000304.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000305.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000306.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000307.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000308.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000309.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000310.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000311.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000312.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000313.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000314.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000315.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000316.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000317.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000318.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000319.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000320.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000321.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000322.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000323.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000324.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000325.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000326.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000327.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000328.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000329.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000330.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000331.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000332.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000333.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000334.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000335.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000336.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000337.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000338.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000339.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000340.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000341.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000342.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000343.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000344.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000345.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000346.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000347.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000348.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000349.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000350.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000351.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000352.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000353.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000354.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000355.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000356.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000357.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000358.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000359.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000360.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000361.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000362.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000363.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000364.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000365.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000366.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000367.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000368.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000369.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000370.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000371.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000372.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000373.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000374.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000375.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000376.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000377.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000378.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000379.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000380.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000381.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000382.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000383.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000384.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000385.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000386.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000387.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000388.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000389.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000390.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000391.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000392.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000393.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000394.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000395.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000396.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000397.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000398.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000399.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000400.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000401.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000402.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000403.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000404.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000405.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000406.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000407.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000408.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000409.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000410.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000411.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000412.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000413.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000414.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000415.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000416.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000417.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000418.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000419.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000420.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000421.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000422.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000423.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000424.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000425.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000426.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000427.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000428.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000429.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000430.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000431.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000432.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000433.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000434.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000435.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000436.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000437.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000438.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000439.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000440.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000441.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000442.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000443.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000444.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000445.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000446.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000447.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000448.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000449.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000450.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000451.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000452.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000453.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000454.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000455.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000456.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000457.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000458.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000459.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000460.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000461.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000462.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000463.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000464.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000465.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000466.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000467.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000468.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000469.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000470.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000471.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000472.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000473.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000474.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000475.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000476.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000477.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000478.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000479.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000480.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000481.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000482.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000483.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000484.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000485.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000486.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000487.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000488.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000489.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000490.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000491.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000492.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000493.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000494.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000495.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000496.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000497.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000498.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000499.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000500.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000501.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000502.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000503.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000504.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000505.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000506.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000507.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000508.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000509.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000510.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000511.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000512.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000513.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000514.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000515.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000516.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000517.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000518.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000519.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000520.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000521.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000522.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000523.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000524.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000525.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000526.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000527.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000528.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000529.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000530.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000531.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000532.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000533.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000534.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000535.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000536.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000537.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000538.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000539.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000540.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000541.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000542.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000543.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000544.swedentfrecord',\n",
      " '/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000545.swedentfrecord']\n",
      "\n",
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/slim/python/slim/data/parallel_reader.py:246: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.596531 139898556168000 deprecation.py:323] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/slim/python/slim/data/parallel_reader.py:246: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.604600 139898556168000 deprecation.py:323] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.606816 139898556168000 deprecation.py:323] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.609724 139898556168000 deprecation.py:323] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.612179 139898556168000 deprecation.py:323] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/slim/python/slim/data/parallel_reader.py:95: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.620273 139898556168000 deprecation.py:323] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/slim/python/slim/data/parallel_reader.py:95: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Tensor(\"fullseeingthroughfogdataset_data_provider/case/cond/Merge:0\", shape=(?, ?, 3), dtype=uint8, device=/device:CPU:0)\n",
      "Shape: Tensor(\"fullseeingthroughfogdataset_data_provider/Reshape_2:0\", shape=(3,), dtype=int64, device=/device:CPU:0)\n",
      "glabels: Tensor(\"fullseeingthroughfogdataset_data_provider/SparseToDense:0\", shape=(?,), dtype=int64, device=/device:CPU:0)\n",
      "gbboxes: Tensor(\"fullseeingthroughfogdataset_data_provider/transpose:0\", shape=(?, 4), dtype=float32, device=/device:CPU:0)\n",
      "WARNING:tensorflow:From /home/saket/Dense/SSD/preprocessing/ssd_vgg_preprocessing.py:101: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.700066 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/preprocessing/ssd_vgg_preprocessing.py:101: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/preprocessing/ssd_vgg_preprocessing.py:219: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.704546 139898556168000 deprecation.py:323] From /home/saket/Dense/SSD/preprocessing/ssd_vgg_preprocessing.py:219: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/tf_extended/math.py:38: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.763853 139898556168000 deprecation.py:323] From /home/saket/Dense/SSD/tf_extended/math.py:38: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/preprocessing/tf_image.py:275: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.794939 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/preprocessing/tf_image.py:275: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/preprocessing/ssd_vgg_preprocessing.py:116: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.823558 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/preprocessing/ssd_vgg_preprocessing.py:116: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "Image: Tensor(\"ssd_preprocessing_train/transpose:0\", shape=(3, 512, 512), dtype=float32, device=/device:CPU:0)\n",
      "glabels: Tensor(\"ssd_preprocessing_train/distorted_bounding_box_crop/bboxes_filter/boolean_mask/GatherV2:0\", shape=(?,), dtype=int64, device=/device:CPU:0)\n",
      "gbboxes: Tensor(\"ssd_preprocessing_train/random_flip_left_right/cond_1/Merge:0\", shape=(?, 4), dtype=float32, device=/device:CPU:0)\n",
      "WARNING:tensorflow:From /home/saket/Dense/SSD/nets/ssd_common.py:77: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.953328 139898556168000 deprecation.py:323] From /home/saket/Dense/SSD/nets/ssd_common.py:77: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/nets/ssd_common.py:152: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:24.993057 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/nets/ssd_common.py:152: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##\n",
      "gclasses: [<tf.Tensor 'bboxes_encode_block_0/while/Exit_1:0' shape=(64, 64, 4) dtype=int64>, <tf.Tensor 'bboxes_encode_block_1/while/Exit_1:0' shape=(32, 32, 6) dtype=int64>, <tf.Tensor 'bboxes_encode_block_2/while/Exit_1:0' shape=(16, 16, 6) dtype=int64>, <tf.Tensor 'bboxes_encode_block_3/while/Exit_1:0' shape=(8, 8, 6) dtype=int64>, <tf.Tensor 'bboxes_encode_block_4/while/Exit_1:0' shape=(4, 4, 6) dtype=int64>, <tf.Tensor 'bboxes_encode_block_5/while/Exit_1:0' shape=(2, 2, 4) dtype=int64>, <tf.Tensor 'bboxes_encode_block_6/while/Exit_1:0' shape=(1, 1, 4) dtype=int64>]\n",
      "glocalization: [<tf.Tensor 'bboxes_encode_block_0/stack:0' shape=(64, 64, 4, 4) dtype=float32>, <tf.Tensor 'bboxes_encode_block_1/stack:0' shape=(32, 32, 6, 4) dtype=float32>, <tf.Tensor 'bboxes_encode_block_2/stack:0' shape=(16, 16, 6, 4) dtype=float32>, <tf.Tensor 'bboxes_encode_block_3/stack:0' shape=(8, 8, 6, 4) dtype=float32>, <tf.Tensor 'bboxes_encode_block_4/stack:0' shape=(4, 4, 6, 4) dtype=float32>, <tf.Tensor 'bboxes_encode_block_5/stack:0' shape=(2, 2, 4, 4) dtype=float32>, <tf.Tensor 'bboxes_encode_block_6/stack:0' shape=(1, 1, 4, 4) dtype=float32>]\n",
      "gscores: [<tf.Tensor 'bboxes_encode_block_0/while/Exit_2:0' shape=(64, 64, 4) dtype=float32>, <tf.Tensor 'bboxes_encode_block_1/while/Exit_2:0' shape=(32, 32, 6) dtype=float32>, <tf.Tensor 'bboxes_encode_block_2/while/Exit_2:0' shape=(16, 16, 6) dtype=float32>, <tf.Tensor 'bboxes_encode_block_3/while/Exit_2:0' shape=(8, 8, 6) dtype=float32>, <tf.Tensor 'bboxes_encode_block_4/while/Exit_2:0' shape=(4, 4, 6) dtype=float32>, <tf.Tensor 'bboxes_encode_block_5/while/Exit_2:0' shape=(2, 2, 4) dtype=float32>, <tf.Tensor 'bboxes_encode_block_6/while/Exit_2:0' shape=(1, 1, 4) dtype=float32>]\n",
      "WARNING:tensorflow:From <ipython-input-1-a5c4cd3199c3>:268: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:25.642459 139898556168000 deprecation.py:323] From <ipython-input-1-a5c4cd3199c3>:268: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/deployment/model_deploy.py:194: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:25.657458 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/deployment/model_deploy.py:194: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/deployment/model_deploy.py:194: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:25.659054 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/deployment/model_deploy.py:194: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:25.663528 139898556168000 deprecation.py:323] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/nets/ssd_vgg_512.py:579: The name tf.losses.compute_weighted_loss is deprecated. Please use tf.compat.v1.losses.compute_weighted_loss instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:26.242242 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/nets/ssd_vgg_512.py:579: The name tf.losses.compute_weighted_loss is deprecated. Please use tf.compat.v1.losses.compute_weighted_loss instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/nets/ssd_vgg_512.py:604: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:26.716893 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/nets/ssd_vgg_512.py:604: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/tf_utils.py:105: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:26.939909 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/tf_utils.py:105: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/tf_utils.py:162: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:26.946065 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/tf_utils.py:162: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/Dense/SSD/tf_utils.py:245: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:26.949170 139898556168000 module_wrapper.py:139] From /home/saket/Dense/SSD/tf_utils.py:245: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:28.029665 139898556168000 deprecation.py:506] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0323 12:28:28.889024 139898556168000 deprecation.py:323] From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tfmodel/model.ckpt-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0323 12:28:29.239311 139898556168000 saver.py:1284] Restoring parameters from /tmp/tfmodel/model.ckpt-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
      "\n",
      "Key ssd_512_vgg/block10/conv1x1/biases not found in checkpoint\n",
      "\t [[node save/RestoreV2 (defined at /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
      "\n",
      "Original stack trace for 'save/RestoreV2':\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2895, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3166, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-a5c4cd3199c3>\", line 408, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"<ipython-input-1-a5c4cd3199c3>\", line 390, in main\n",
      "    pad_step_number=False)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n",
      "    self.build()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n",
      "    name=name)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0323 12:28:29.743692 139898556168000 coordinator.py:224] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
      "\n",
      "Key ssd_512_vgg/block10/conv1x1/biases not found in checkpoint\n",
      "\t [[node save/RestoreV2 (defined at /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
      "\n",
      "Original stack trace for 'save/RestoreV2':\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2895, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3166, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-a5c4cd3199c3>\", line 408, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"<ipython-input-1-a5c4cd3199c3>\", line 390, in main\n",
      "    pad_step_number=False)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n",
      "    self.build()\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n",
      "    name=name)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey ssd_512_vgg/block10/conv1x1/biases not found in checkpoint\n\t [[node save/RestoreV2 (defined at /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save/RestoreV2':\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.start()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2895, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3166, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-a5c4cd3199c3>\", line 408, in <module>\n    tf.app.run()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 303, in run\n    _run_main(main, args)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\n    sys.exit(main(argv))\n  File \"<ipython-input-1-a5c4cd3199c3>\", line 390, in main\n    pad_step_number=False)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key ssd_512_vgg/block10/conv1x1/biases not found in checkpoint\n\t [[{{node save/RestoreV2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1290\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1291\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key ssd_512_vgg/block10/conv1x1/biases not found in checkpoint\n\t [[node save/RestoreV2 (defined at /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save/RestoreV2':\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.start()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2895, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3166, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-a5c4cd3199c3>\", line 408, in <module>\n    tf.app.run()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 303, in run\n    _run_main(main, args)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\n    sys.exit(main(argv))\n  File \"<ipython-input-1-a5c4cd3199c3>\", line 390, in main\n    pad_step_number=False)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1299\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m         \u001b[0mnames_to_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m   1617\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m   \u001b[0mobject_graph_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_GRAPH_PROTO_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m   \u001b[0mobject_graph_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrackable_object_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackableObjectGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader_GetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a5c4cd3199c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a5c4cd3199c3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0msave_interval_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_interval_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0msession_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             sync_optimizer=None)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/slim/python/slim/learning.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_op, logdir, train_step_fn, train_step_kwargs, log_every_n_steps, graph, master, is_chief, global_step, number_of_steps, init_op, init_feed_dict, local_init_op, init_fn, ready_op, summary_op, save_summaries_secs, summary_writer, startup_delay_steps, saver, save_interval_secs, sync_optimizer, session_config, session_wrapper, trace_every_n_steps, ignore_live_threads)\u001b[0m\n\u001b[1;32m    751\u001b[0m       \u001b[0mshould_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m       with sv.managed_session(\n\u001b[0;32m--> 753\u001b[0;31m           master, start_standard_services=False, config=session_config) as sess:\n\u001b[0m\u001b[1;32m    754\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msession_wrapper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;31m# threads which are not checking for `should_stop()`.  They\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;31m# will be stopped when we close the session further down.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;31m# Close the session to finish up all pending calls.  We do not care\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/supervisor.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, threads, close_summary_writer, ignore_live_threads)\u001b[0m\n\u001b[1;32m    837\u001b[0m           \u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m           \u001b[0mstop_grace_period_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_grace_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m           ignore_live_threads=ignore_live_threads)\n\u001b[0m\u001b[1;32m    840\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m       \u001b[0;31m# Close the writer last, in case one of the running threads was using it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mstragglers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_live_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m   1001\u001b[0m           \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m           start_standard_services=start_standard_services)\n\u001b[0m\u001b[1;32m   1004\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/supervisor.py\u001b[0m in \u001b[0;36mprepare_or_wait_for_session\u001b[0;34m(self, master, config, wait_for_checkpoint, max_wait_secs, start_standard_services)\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m           \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m           init_fn=self._init_fn)\n\u001b[0m\u001b[1;32m    735\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstart_standard_services\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         config=config)\n\u001b[0m\u001b[1;32m    291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minit_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_init_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/session_manager.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# Loads the checkpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecover_last_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_model_checkpoint_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1306\u001b[0;31m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m       \u001b[0;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey ssd_512_vgg/block10/conv1x1/biases not found in checkpoint\n\t [[node save/RestoreV2 (defined at /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save/RestoreV2':\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.start()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2895, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3166, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-a5c4cd3199c3>\", line 408, in <module>\n    tf.app.run()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 303, in run\n    _run_main(main, args)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\n    sys.exit(main(argv))\n  File \"<ipython-input-1-a5c4cd3199c3>\", line 390, in main\n    pad_step_number=False)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 Paul Balanca. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Generic training script that trains a SSD model using a given dataset.\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "\n",
    "from datasets import dataset_factory\n",
    "from deployment import model_deploy\n",
    "from nets import nets_factory\n",
    "from preprocessing import preprocessing_factory\n",
    "import tf_utils\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "#device= 'cpu'\n",
    "DATA_FORMAT = 'NCHW'\n",
    "print(\"Num GPUs Available: \", len( tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# =========================================================================== #\n",
    "# SSD Network flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'loss_alpha', 1., 'Alpha parameter in the loss function.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'negative_ratio', 3., 'Negative ratio in the loss function.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'match_threshold', 0.5, 'Matching threshold in the loss function.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# General Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'train_dir', '/tmp/tfmodel/',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "tf.app.flags.DEFINE_integer('num_clones', 1,\n",
    "                            'Number of model clones to deploy.')\n",
    "tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\n",
    "                            'Use CPUs to deploy clones.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_readers', 4,\n",
    "    'The number of parallel readers that read data from the dataset.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_preprocessing_threads', 4,\n",
    "    'The number of threads used to create the batches.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'log_every_n_steps', 10,\n",
    "    'The frequency with which logs are print.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_summaries_secs', 600,\n",
    "    'The frequency with which summaries are saved, in seconds.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_interval_secs', 600,\n",
    "    'The frequency with which the model is saved, in seconds.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'gpu_memory_fraction', 0.8, 'GPU memory fraction to use.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Optimization Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'weight_decay', 0.00004, 'The weight decay on the model weights.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'optimizer', 'rmsprop',\n",
    "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n",
    "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adadelta_rho', 0.95,\n",
    "    'The decay rate for adadelta.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adagrad_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the AdaGrad accumulators.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta1', 0.9,\n",
    "    'The exponential decay rate for the 1st moment estimates.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta2', 0.999,\n",
    "    'The exponential decay rate for the 2nd moment estimates.')\n",
    "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\n",
    "tf.app.flags.DEFINE_float('ftrl_learning_rate_power', -0.5,\n",
    "                          'The learning rate power.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the FTRL accumulators.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l1', 0.0, 'The FTRL l1 regularization strength.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l2', 0.0, 'The FTRL l2 regularization strength.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'momentum', 0.9,\n",
    "    'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_momentum', 0.9, 'Momentum.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_decay', 0.9, 'Decay term for RMSProp.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Learning Rate Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'learning_rate_decay_type',\n",
    "    'exponential',\n",
    "    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\n",
    "    ' or \"polynomial\"')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'end_learning_rate', 0.0001,\n",
    "    'The minimal end learning rate used by a polynomial decay learning rate.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'label_smoothing', 0.0, 'The amount of label smoothing.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'num_epochs_per_decay', 2.0,\n",
    "    'Number of epochs after which learning rate decays.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'moving_average_decay', None,\n",
    "    'The decay to use for the moving average.'\n",
    "    'If left as None, then moving averages are not used.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Dataset Flags.\n",
    "# =========================================================================== #\n",
    "# change\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_name', 'fullseeingthroughfogdataset', 'The name of the dataset to load.')\n",
    "# change\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_classes', 8, 'Number of classes to use in the dataset.')\n",
    "# change\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_split_name', 'train_clear_day', 'The name of the train/test split.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_dir', None, 'The directory where the dataset files are stored.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'labels_offset', 0,\n",
    "    'An offset for the labels in the dataset. This flag is primarily used to '\n",
    "    'evaluate the VGG and ResNet architectures which do not use a background '\n",
    "    'class for the ImageNet dataset.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'model_name', 'ssd_512_vgg', 'The name of the architecture to train.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'preprocessing_name', None, 'The name of the preprocessing to use. If left '\n",
    "    'as `None`, then the model_name flag is used.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'batch_size', 32, 'The number of samples in each batch.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'train_image_size', None, 'Train image size')\n",
    "tf.app.flags.DEFINE_integer('max_number_of_steps', None,\n",
    "                            'The maximum number of training steps.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Fine-Tuning Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_path', None,\n",
    "    'The path to a checkpoint from which to fine-tune.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_model_scope', None,\n",
    "    'Model scope in the checkpoint. None if the same as the trained model.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_exclude_scopes', None,\n",
    "    'Comma-separated list of scopes of variables to exclude when restoring '\n",
    "    'from a checkpoint.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'trainable_scopes', None,\n",
    "    'Comma-separated list of scopes to filter the set of variables to train.'\n",
    "    'By default, None would train all the variables.')\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    'ignore_missing_vars', False,\n",
    "    'When restoring a checkpoint would ignore missing variables.')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "# =========================================================================== #\n",
    "# Main training routine.\n",
    "# =========================================================================== #\n",
    "def main(_):\n",
    "    FLAGS.dataset_dir = \"/data/datasets/saket/SeeingThroughFogData/train_clear_day\"\n",
    "    if not FLAGS.dataset_dir:\n",
    "        raise ValueError('You must supply the dataset directory with --dataset_dir')\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "    with tf.Graph().as_default():\n",
    "        # Config model_deploy. Keep TF Slim Models structure.\n",
    "        # Useful if want to need multiple GPUs and/or servers in the future.\n",
    "        deploy_config = model_deploy.DeploymentConfig(\n",
    "            num_clones=FLAGS.num_clones,\n",
    "            clone_on_cpu=FLAGS.clone_on_cpu,\n",
    "            replica_id=0,\n",
    "            num_replicas=1,\n",
    "            num_ps_tasks=0)\n",
    "        # Create global_step.\n",
    "        with tf.device(deploy_config.variables_device()):\n",
    "            global_step = slim.create_global_step()\n",
    "\n",
    "        # Select the dataset.\n",
    "        dataset = dataset_factory.get_dataset(\n",
    "            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n",
    "\n",
    "        # Get the SSD network and its anchors.\n",
    "        ssd_class = nets_factory.get_network(FLAGS.model_name)\n",
    "        ssd_params = ssd_class.default_params._replace(num_classes=FLAGS.num_classes)\n",
    "        ssd_net = ssd_class(ssd_params)\n",
    "        ssd_shape = ssd_net.params.img_shape\n",
    "        ssd_anchors = ssd_net.anchors(ssd_shape)\n",
    "\n",
    "        # Select the preprocessing function.\n",
    "        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n",
    "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n",
    "            preprocessing_name, is_training=True)\n",
    "\n",
    "        tf_utils.print_configuration(FLAGS.__flags, ssd_params,\n",
    "                                     dataset.data_sources, FLAGS.train_dir)\n",
    "        # =================================================================== #\n",
    "        # Create a dataset provider and batches.\n",
    "        # =================================================================== #\n",
    "        with tf.device(deploy_config.inputs_device()):\n",
    "            with tf.name_scope(FLAGS.dataset_name + '_data_provider'):\n",
    "                provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "                    dataset,\n",
    "                    num_readers=FLAGS.num_readers,\n",
    "                    common_queue_capacity=20 * FLAGS.batch_size,\n",
    "                    common_queue_min=10 * FLAGS.batch_size,\n",
    "                    shuffle=True)\n",
    "            # Get for SSD network: image, labels, bboxes.\n",
    "            # change\n",
    "            [image, shape, glabels, gbboxes] = provider.get(['image', 'shape',\n",
    "                                                             'class/text',\n",
    "                                                             'object/bbox'])\n",
    "            print(\"Image:\",image)\n",
    "            print(\"Shape:\",shape)\n",
    "            print(\"glabels:\", glabels)\n",
    "            print(\"gbboxes:\",gbboxes)\n",
    "            # Pre-processing image, labels and bboxes.\n",
    "            image, glabels, gbboxes = \\\n",
    "                image_preprocessing_fn(image, glabels, gbboxes,\n",
    "                                       out_shape=ssd_shape,\n",
    "                                       data_format=DATA_FORMAT)\n",
    "            print('***')\n",
    "            print(\"Image:\",image)\n",
    "            #print(\"Shape:\",shape)\n",
    "            print(\"glabels:\", glabels)\n",
    "            print(\"gbboxes:\",gbboxes)\n",
    "            # Encode groundtruth labels and bboxes.\n",
    "            gclasses, glocalisations, gscores = \\\n",
    "                ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)\n",
    "            batch_shape = [1] + [len(ssd_anchors)] * 3\n",
    "            print('##')\n",
    "            print('gclasses:', gclasses)\n",
    "            print('glocalization:', glocalisations)\n",
    "            print('gscores:', gscores)\n",
    "            # Training batches and queue.\n",
    "            r = tf.train.batch(\n",
    "                tf_utils.reshape_list([image, gclasses, glocalisations, gscores]),\n",
    "                batch_size=FLAGS.batch_size,\n",
    "                num_threads=FLAGS.num_preprocessing_threads,\n",
    "                capacity=5 * FLAGS.batch_size)\n",
    "            b_image, b_gclasses, b_glocalisations, b_gscores = \\\n",
    "                tf_utils.reshape_list(r, batch_shape)\n",
    "\n",
    "            # Intermediate queueing: unique batch computation pipeline for all\n",
    "            # GPUs running the training.\n",
    "            batch_queue = slim.prefetch_queue.prefetch_queue(\n",
    "                tf_utils.reshape_list([b_image, b_gclasses, b_glocalisations, b_gscores]),\n",
    "                capacity=2 * deploy_config.num_clones)\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Define the model running on every GPU.\n",
    "        # =================================================================== #\n",
    "        def clone_fn(batch_queue):\n",
    "            \"\"\"Allows data parallelism by creating multiple\n",
    "            clones of network_fn.\"\"\"\n",
    "            # Dequeue batch.\n",
    "            b_image, b_gclasses, b_glocalisations, b_gscores = \\\n",
    "                tf_utils.reshape_list(batch_queue.dequeue(), batch_shape)\n",
    "\n",
    "            # Construct SSD network.\n",
    "            arg_scope = ssd_net.arg_scope(weight_decay=FLAGS.weight_decay,\n",
    "                                          data_format=DATA_FORMAT)\n",
    "            with slim.arg_scope(arg_scope):\n",
    "                predictions, localisations, logits, end_points = \\\n",
    "                    ssd_net.net(b_image, is_training=True)\n",
    "            # Add loss function.\n",
    "            ssd_net.losses(logits, localisations,\n",
    "                           b_gclasses, b_glocalisations, b_gscores,\n",
    "                           match_threshold=FLAGS.match_threshold,\n",
    "                           negative_ratio=FLAGS.negative_ratio,\n",
    "                           alpha=FLAGS.loss_alpha,\n",
    "                           label_smoothing=FLAGS.label_smoothing)\n",
    "            return end_points\n",
    "\n",
    "        # Gather initial summaries.\n",
    "        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Add summaries from first clone.\n",
    "        # =================================================================== #\n",
    "        clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n",
    "        first_clone_scope = deploy_config.clone_scope(0)\n",
    "        # Gather update_ops from the first clone. These contain, for example,\n",
    "        # the updates for the batch_norm variables created by network_fn.\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n",
    "\n",
    "        # Add summaries for end_points.\n",
    "        end_points = clones[0].outputs\n",
    "        for end_point in end_points:\n",
    "            x = end_points[end_point]\n",
    "            summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
    "            summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
    "                                            tf.nn.zero_fraction(x)))\n",
    "        # Add summaries for losses and extra losses.\n",
    "        for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n",
    "            summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "        for loss in tf.get_collection('EXTRA_LOSSES', first_clone_scope):\n",
    "            summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "        # Add summaries for variables.\n",
    "        for variable in slim.get_model_variables():\n",
    "            summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Configure the moving averages.\n",
    "        # =================================================================== #\n",
    "        if FLAGS.moving_average_decay:\n",
    "            moving_average_variables = slim.get_model_variables()\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(\n",
    "                FLAGS.moving_average_decay, global_step)\n",
    "        else:\n",
    "            moving_average_variables, variable_averages = None, None\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Configure the optimization procedure.\n",
    "        # =================================================================== #\n",
    "        with tf.device(deploy_config.optimizer_device()):\n",
    "            learning_rate = tf_utils.configure_learning_rate(FLAGS,\n",
    "                                                             dataset.num_samples,\n",
    "                                                             global_step)\n",
    "            optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n",
    "            summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "        if FLAGS.moving_average_decay:\n",
    "            # Update ops executed locally by trainer.\n",
    "            update_ops.append(variable_averages.apply(moving_average_variables))\n",
    "\n",
    "        # Variables to train.\n",
    "        variables_to_train = tf_utils.get_variables_to_train(FLAGS)\n",
    "\n",
    "        # and returns a train_tensor and summary_op\n",
    "        total_loss, clones_gradients = model_deploy.optimize_clones(\n",
    "            clones,\n",
    "            optimizer,\n",
    "            var_list=variables_to_train)\n",
    "        # Add total_loss to summary.\n",
    "        summaries.add(tf.summary.scalar('total_loss', total_loss))\n",
    "\n",
    "        # Create gradient updates.\n",
    "        grad_updates = optimizer.apply_gradients(clones_gradients,\n",
    "                                                 global_step=global_step)\n",
    "        update_ops.append(grad_updates)\n",
    "        update_op = tf.group(*update_ops)\n",
    "        train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n",
    "                                                          name='train_op')\n",
    "\n",
    "        # Add the summaries from the first clone. These contain the summaries\n",
    "        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n",
    "                                           first_clone_scope))\n",
    "        # Merge all summaries together.\n",
    "        summary_op = tf.summary.merge(list(summaries), name='summary_op')\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Kicks off the training.\n",
    "        # =================================================================== #\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n",
    "        config = tf.ConfigProto(log_device_placement=False,\n",
    "                                gpu_options=gpu_options)\n",
    "        saver = tf.train.Saver(max_to_keep=5,\n",
    "                               keep_checkpoint_every_n_hours=1.0,\n",
    "                               write_version=2,\n",
    "                               pad_step_number=False)\n",
    "        slim.learning.train(\n",
    "            train_tensor,\n",
    "            logdir=FLAGS.train_dir,\n",
    "            master='',\n",
    "            is_chief=True,\n",
    "            init_fn=tf_utils.get_init_fn(FLAGS),\n",
    "            summary_op=summary_op,\n",
    "            number_of_steps=FLAGS.max_number_of_steps,\n",
    "            log_every_n_steps=FLAGS.log_every_n_steps,\n",
    "            save_summaries_secs=FLAGS.save_summaries_secs,\n",
    "            saver=saver,\n",
    "            save_interval_secs=FLAGS.save_interval_secs,\n",
    "            session_config=config,\n",
    "            sync_optimizer=None)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "material-muslim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFRecord Files: 491\n",
      "Validation TFRecord Files: 55\n"
     ]
    }
   ],
   "source": [
    "FILENAMES = tf.io.gfile.glob(GCS_PATH)\n",
    "split_ind = int(0.9 * len(FILENAMES))\n",
    "TRAINING_FILENAMES, VALID_FILENAMES = FILENAMES[:split_ind], FILENAMES[split_ind:]\n",
    "print(\"Train TFRecord Files:\", len(TRAINING_FILENAMES))\n",
    "print(\"Validation TFRecord Files:\", len(VALID_FILENAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "southern-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    #image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fantastic-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(example, labeled):\n",
    "    tfrecord_format = (\n",
    "        {'image/cam_stereo_left_lut': tf.io.FixedLenFeature([],tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string, default_value='png'),\n",
    "        'image/shape/cam_stereo_left_lut': tf.FixedLenFeature([3], tf.int64),\n",
    "        'image/object/class/text': tf.io.VarLenFeature(dtype=tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        }\n",
    "    )\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example[\"image/cam_stereo_left_lut\"])\n",
    "    if labeled:\n",
    "        label = tf.cast(example[\"image/object/class/text\"], tf.string)\n",
    "        #colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])\n",
    "        bbox_y_min = tf.cast(\"image/object/bbox/y_min\", tf.float32)\n",
    "        bbox_x_min = tf.cast(\"image/object/bbox/x_min\", tf.float32)  \n",
    "        bbox_y_max = tf.cast(\"image/object/bbox/y_max\", tf.float32)\n",
    "        bbox_x_max = tf.cast(\"image/object/bbox/x_max\", tf.float32)\n",
    "        #bbox = [bbox_y_min,bbox_x_min,bbox_y_max, bbox_x_max]\n",
    "        #bbox = example[bbox]\n",
    "        #bbox = tf.cast(example[(\"image/object/bbox/y_min\",\"image/object/bbox/x_min\",\n",
    "        #                       \"image/object/bbox/y_max\",\"image/object/bbox/x_max\")],\n",
    "        #               tf.int32)\n",
    "\n",
    "        return image, label\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dependent-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, labeled=True):\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        filenames\n",
    "    )  # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(\n",
    "        ignore_order\n",
    "    )  # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(\n",
    "        partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or just images if labeled=False\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "satellite-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_dataset(filenames, labeled=True):\n",
    "    dataset = load_dataset(filenames, labeled=labeled)\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demonstrated-damages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.framework.ops.enable_eager_execution(config=None, device_policy=None, execution_mode=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = get_dataset(TRAINING_FILENAMES)\n",
    "valid_dataset = get_dataset(VALID_FILENAMES)\n",
    "tf.enable_eager_execution\n",
    "#image_batch, label_batch = next(iter(valid_dataset))\n",
    "\n",
    "#len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adjacent-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"melanoma_model.h5\", save_best_only=True\n",
    ")\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eastern-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "   # base_model = tf.keras.applications.Xception(\n",
    "    #    input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\"\n",
    "    #)\n",
    "\n",
    "    #base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input([*IMAGE_SIZE, 3])\n",
    "    x = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    #x = base_model(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=\"binary_crossentropy\",\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "literary-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker CollectiveAllReduceStrategy with local_devices = ('/device:CPU:0',), communication = CollectiveCommunication.AUTO\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "significant-stopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-efficientnet in /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages (0.1.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "buried-college",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental.preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-56c9b15a6e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import tensorflow as tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#from tf.keras.application import efficientnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     raise ImportError(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "#from tf.keras.application import efficientnet\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "distinct-senator",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v1.keras.applications' has no attribute 'EfficientNetB0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-37a12eb97fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model = tf.keras.applications.EfficientNetB0(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m91\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v1.keras.applications' has no attribute 'EfficientNetB0'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_filenames = tf.io.gfile.glob(f\"/data/datasets/saket/SeeingThroughFogData/train_clear_day/*.swedentfrecord\")\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "steps_per_epoch = 50\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "input_tensor = tf.keras.layers.Input(shape=(224, 224, 3), name=\"image\")\n",
    "model = tf.keras.applications.EfficientNetB0(\n",
    "    input_tensor=input_tensor, weights=None, classes=91\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x=get_dataset(train_filenames, batch_size),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "higher-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/saket/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on None steps\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [64,1] vs. [703,2]\n\t [[{{node training/Adam/gradients/gradients/loss_1/dense_1_loss/logistic_loss/mul_grad/BroadcastGradientArgs}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7838b107e38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [64,1] vs. [703,2]\n\t [[{{node training/Adam/gradients/gradients/loss_1/dense_1_loss/logistic_loss/mul_grad/BroadcastGradientArgs}}]]"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = make_model()\n",
    "\n",
    "history = model.fit(\n",
    "    valid_dataset,\n",
    "    epochs=2,\n",
    "    validation_data=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-research",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dying-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, path, example):\n",
    "    feature = {\n",
    "        \"image\": image_feature(image),\n",
    "        \"path\": bytes_feature(path),\n",
    "        \"area\": float_feature(example[\"area\"]),\n",
    "        \"bbox\": float_feature_list(example[\"bbox\"]),\n",
    "        \"category_id\": int64_feature(example[\"category_id\"]),\n",
    "        \"id\": int64_feature(example[\"id\"]),\n",
    "        \"image_id\": int64_feature(example[\"image_id\"]),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "       'image/format': tf.io.FixedLenFeature((), tf.string, default_value='png'),\n",
    "        #'image/object/class/text': tf.io.FixedLenFeature((), tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/angle': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/truncation': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/occlusion': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/object/bbox3d/height': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/width': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/length': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/x': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/y': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/z': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox3d/alpha3d': tf.io.VarLenFeature(dtype=tf.float32),\n",
    "        'image/shape': tf.io.FixedLenFeature([3], tf.int64),\n",
    "        }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image/format\"], channels=3)\n",
    "    #example[\"label\"] = tf.sparse.to_denset(example[\"image/object/class/text\"], tf.int32)\n",
    "    example[\"bbox_y_min\"] = tf.sparse.to_dense(example[\"image/object/bbox/ymin\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "tired-algebra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function parse_tfrecord_fn at 0x7fd7281f8b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse_tfrecord_fn at 0x7fd7281f8b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Feature: image/shape (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2112\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Feature: image/shape (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-52540b816e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparsed_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_tfrecord_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Feature: image/shape (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]"
     ]
    }
   ],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(\"/data/datasets/saket/SeeingThroughFogData/train_clear_day/train_clear_day_000000.swedentfrecord\")\n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord_fn)\n",
    "\n",
    "for features in parsed_dataset.take(1):\n",
    "    for key in features.keys():\n",
    "        if key != \"image\":\n",
    "            print(f\"{key}: {features[key]}\")\n",
    "\n",
    "    print(f\"Image shape: {features['image'].shape}\")\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(features[\"image\"].numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-humidity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
